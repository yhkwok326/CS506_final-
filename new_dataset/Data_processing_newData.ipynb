{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eb1cf76-424a-47f5-af97-7c515ec66802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and normalizing dataset...\n",
      "Processing train/Mild_Impairment...\n",
      "Processing train/Moderate_Impairment...\n",
      "Processing train/No_Impairment...\n",
      "Processing train/Very_Mild_Impairment...\n",
      "Processing test/Mild_Impairment...\n",
      "Processing test/Moderate_Impairment...\n",
      "Processing test/No_Impairment...\n",
      "Processing test/Very_Mild_Impairment...\n",
      "Processed 10240 training images and 1279 test images\n",
      "\n",
      "Training set class distribution:\n",
      "  Mild_Impairment: 2560 images (25.0%)\n",
      "  Moderate_Impairment: 2560 images (25.0%)\n",
      "  No_Impairment: 2560 images (25.0%)\n",
      "  Very_Mild_Impairment: 2560 images (25.0%)\n",
      "\n",
      "Test set class distribution:\n",
      "  Mild_Impairment: 179 images (14.0%)\n",
      "  Moderate_Impairment: 12 images (0.9%)\n",
      "  No_Impairment: 640 images (50.0%)\n",
      "  Very_Mild_Impairment: 448 images (35.0%)\n",
      "\n",
      "Generating visualization...\n",
      "\n",
      "Saving normalized dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving train images: 100%|███████████████| 10240/10240 [00:13<00:00, 732.48it/s]\n",
      "Saving test images: 100%|██████████████████| 1279/1279 [00:01<00:00, 726.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 11519 images processed and saved with normalizations\n",
      "\n",
      "Feature extraction complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset paths\n",
    "folder_train_path = \"extracted_data/Combined_dataset/train/\"\n",
    "folder_test_path = \"extracted_data/Combined_dataset/test/\"\n",
    "\n",
    "# Output path for combined dataset\n",
    "output_dir = \"N_Combined_MRI_Dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"test\"), exist_ok=True)\n",
    "\n",
    "# Settings\n",
    "img_size = 128  # set 128 for now, might up to 256\n",
    "\n",
    "# Class mapping - Aligning with the shared code\n",
    "classes = {\n",
    "    'Mild_Impairment': 0,\n",
    "    'Moderate_Impairment': 1,\n",
    "    'No_Impairment': 2,\n",
    "    'Very_Mild_Impairment': 3\n",
    "}\n",
    "\n",
    "def load_and_normalize_dataset(folder_train_path, folder_test_path):\n",
    "    normalized_data = []\n",
    "    \n",
    "    # Process train and test folders\n",
    "    for split, path in [('train', folder_train_path), ('test', folder_test_path)]:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: {path} does not exist, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Process each class folder\n",
    "        for class_name, label in classes.items():\n",
    "            class_dir = os.path.join(path, class_name)\n",
    "            \n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"Warning: Class directory {class_dir} does not exist, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Process each image in the class folder\n",
    "            print(f\"Processing {split}/{class_name}...\")\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_dir, img_file)\n",
    "                    \n",
    "                    try:\n",
    "                        # Load image\n",
    "                        img = Image.open(img_path).convert('L')\n",
    "                        img_array = np.array(img) / 255.0  # Normalize to [0,1]\n",
    "                        \n",
    "                        # Resize to standard size\n",
    "                        img_resized = cv2.resize(img_array, (img_size, img_size))\n",
    "                        \n",
    "                        # Process and store\n",
    "                        item = {\n",
    "                            'original_path': img_path,\n",
    "                            'label': label,\n",
    "                            'class_name': class_name,\n",
    "                            'split': split,\n",
    "                            'image': img_resized,\n",
    "                            'dataset': 'folder_dataset' if 'folder' in img_file else 'parquet_dataset'\n",
    "                        }\n",
    "                        \n",
    "                        # Apply normalizations\n",
    "                        normalized_item = normalize_mri_for_ventricles(item)\n",
    "                        normalized_data.append(normalized_item)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    # Split back into train and test\n",
    "    train_data = [item for item in normalized_data if item['split'] == 'train']\n",
    "    test_data = [item for item in normalized_data if item['split'] == 'test']\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def normalize_mri_for_ventricles(item):\n",
    "    image = item['image']\n",
    "    \n",
    "    # 1. Simple normalization - just use the original normalized image\n",
    "    item['image_normalized'] = image\n",
    "    \n",
    "    # 2. Ventricle enhancement\n",
    "    \n",
    "    # Create version optimized for dark ventricle regions\n",
    "    img_uint8 = (item['image_normalized'] * 255).astype(np.uint8)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(img_uint8)\n",
    "    item['image_enhanced'] = enhanced / 255.0\n",
    "    \n",
    "    # Create inverted version to highlight ventricles\n",
    "    inverted = 1 - item['image_normalized']\n",
    "    # Apply adaptive thresholding to highlight ventricle regions\n",
    "    item['image_ventricle_focus'] = inverted\n",
    "    \n",
    "    # 3. Ventricle segmentation using Otsu thresholding\n",
    "    otsu_thresh, _ = cv2.threshold(\n",
    "        img_uint8, \n",
    "        0, \n",
    "        255, \n",
    "        cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU\n",
    "    )\n",
    "    \n",
    "    _, ventricle_mask = cv2.threshold(\n",
    "        img_uint8, \n",
    "        int(otsu_thresh * 0.5),  # may add 50% of the Otsu threshold\n",
    "        255, \n",
    "        cv2.THRESH_BINARY_INV\n",
    "    )\n",
    "    \n",
    "    # Clean up mask\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    ventricle_mask = cv2.morphologyEx(ventricle_mask, cv2.MORPH_OPEN, kernel)\n",
    "    ventricle_mask = cv2.morphologyEx(ventricle_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    item['ventricle_mask'] = ventricle_mask / 255.0\n",
    "    \n",
    "    return item\n",
    "\n",
    "def visualize_normalizations(data, num_samples=4):\n",
    "    # Select samples from each class if possible\n",
    "    samples = []\n",
    "    class_names = set(item['class_name'] for item in data)\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_items = [item for item in data if item['class_name'] == class_name]\n",
    "        if class_items:\n",
    "            samples.append(class_items[0])\n",
    "            if len(samples) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Use random samples if we don't have enough\n",
    "    if len(samples) < num_samples:\n",
    "        remaining = [i for i in data if not any(s['original_path'] == i['original_path'] for s in samples)]\n",
    "        if remaining:\n",
    "            additional = np.random.choice(\n",
    "                remaining,\n",
    "                size=min(num_samples - len(samples), len(remaining)),\n",
    "                replace=False\n",
    "            ).tolist()\n",
    "            samples.extend(additional)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(len(samples), 5, figsize=(20, 4 * len(samples)))\n",
    "    \n",
    "    # Handle case with just one sample\n",
    "    if len(samples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, item in enumerate(samples):\n",
    "        # Original\n",
    "        axes[i][0].imshow(item['image'], cmap='gray')\n",
    "        axes[i][0].set_title(f\"{item['class_name']}\\nOriginal\")\n",
    "        axes[i][0].axis('off')\n",
    "        \n",
    "        # Normalized\n",
    "        axes[i][1].imshow(item['image_normalized'], cmap='gray')\n",
    "        axes[i][1].set_title('Normalized')\n",
    "        axes[i][1].axis('off')\n",
    "        \n",
    "        # Enhanced\n",
    "        axes[i][2].imshow(item['image_enhanced'], cmap='gray')\n",
    "        axes[i][2].set_title('Enhanced (CLAHE)')\n",
    "        axes[i][2].axis('off')\n",
    "        \n",
    "        # Ventricle Focus (Inverted)\n",
    "        axes[i][3].imshow(item['image_ventricle_focus'], cmap='gray')\n",
    "        axes[i][3].set_title('Ventricle Focus')\n",
    "        axes[i][3].axis('off')\n",
    "        \n",
    "        # Ventricle Mask\n",
    "        axes[i][4].imshow(item['ventricle_mask'], cmap='gray')\n",
    "        axes[i][4].set_title('Ventricle Mask')\n",
    "        axes[i][4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def save_normalized_dataset(output_dir, train_data, test_data):\n",
    "    saved_count = 0\n",
    "    \n",
    "    for split, data in [('train', train_data), ('test', test_data)]:\n",
    "        split_dir = os.path.join(output_dir, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # Create class directories\n",
    "        class_names = set(item['class_name'] for item in data)\n",
    "        for class_name in class_names:\n",
    "            os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\n",
    "        \n",
    "        # Save normalized images\n",
    "        for item in tqdm(data, desc=f\"Saving {split} images\"):\n",
    "            # Generate filename\n",
    "            original_filename = os.path.basename(item['original_path'])\n",
    "            base_name = os.path.splitext(original_filename)[0]\n",
    "            \n",
    "            # Define paths for different normalizations\n",
    "            class_dir = os.path.join(split_dir, item['class_name'])\n",
    "            \n",
    "            # Save normalized image\n",
    "            norm_img = (item['image_normalized'] * 255).astype(np.uint8)\n",
    "            norm_path = os.path.join(class_dir, f\"{base_name}_norm.png\")\n",
    "            Image.fromarray(norm_img).save(norm_path)\n",
    "            \n",
    "            # Save enhanced image\n",
    "            enhanced_img = (item['image_enhanced'] * 255).astype(np.uint8)\n",
    "            enhanced_path = os.path.join(class_dir, f\"{base_name}_enhanced.png\")\n",
    "            Image.fromarray(enhanced_img).save(enhanced_path)\n",
    "            \n",
    "            # Save ventricle focused image\n",
    "            ventricle_img = (item['image_ventricle_focus'] * 255).astype(np.uint8)\n",
    "            ventricle_path = os.path.join(class_dir, f\"{base_name}_ventricle.png\")\n",
    "            Image.fromarray(ventricle_img).save(ventricle_path)\n",
    "            \n",
    "            # Save ventricle mask\n",
    "            mask_img = (item['ventricle_mask'] * 255).astype(np.uint8)\n",
    "            mask_path = os.path.join(class_dir, f\"{base_name}_mask.png\")\n",
    "            Image.fromarray(mask_img).save(mask_path)\n",
    "            \n",
    "            saved_count += 1\n",
    "    \n",
    "    print(f\"Total of {saved_count} images processed and saved with normalizations\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and normalizing dataset...\")\n",
    "    train_data, test_data = load_and_normalize_dataset(folder_train_path, folder_test_path)\n",
    "    \n",
    "    print(f\"Processed {len(train_data)} training images and {len(test_data)} test images\")\n",
    "    \n",
    "    # Class distribution summary\n",
    "    for split_name, split_data in [(\"Training\", train_data), (\"Test\", test_data)]:\n",
    "        print(f\"\\n{split_name} set class distribution:\")\n",
    "        class_counts = {}\n",
    "        for item in split_data:\n",
    "            class_name = item['class_name']\n",
    "            if class_name in class_counts:\n",
    "                class_counts[class_name] += 1\n",
    "            else:\n",
    "                class_counts[class_name] = 1\n",
    "        \n",
    "        for class_name, count in class_counts.items():\n",
    "            print(f\"  {class_name}: {count} images ({count/len(split_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize normalizations\n",
    "    print(\"\\nGenerating visualization...\")\n",
    "    fig = visualize_normalizations(train_data)\n",
    "    plt.savefig(os.path.join(output_dir, \"normalization_visualization.png\"))\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Save normalized dataset\n",
    "    print(\"\\nSaving normalized dataset...\")\n",
    "    save_normalized_dataset(output_dir, train_data, test_data)\n",
    "    \n",
    "    print(\"\\nFeature extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c9c4a-81b7-4072-831a-4f36535cc7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
