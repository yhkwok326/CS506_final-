{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f6cc69-0611-4a19-afe0-0e1db5b79f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# Class mapping constants consistent with previous preprocessing\n",
    "N_CLASSES = 4  # Mild_Demented(0), Moderate_Demented(1), Non_Demented(2), Very_Mild_Demented(3)\n",
    "CLASS_NAMES = ['Mild_Demented', 'Moderate_Demented', 'Non_Demented', 'Very_Mild_Demented']\n",
    "\n",
    "# Base directory from original preprocessing\n",
    "base_dir = \"Combined_MRI_Dataset\"\n",
    "output_dir = os.path.join(base_dir, \"model_output\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(output_dir, \"models\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"plots\"), exist_ok=True)\n",
    "\n",
    "# Custom Dataset class for processed ventricle images\n",
    "class VentricleDataset(Dataset):\n",
    "    def __init__(self, base_dir, split='train', normalization_type='ventricle'):\n",
    "       \n",
    "        self.base_dir = base_dir\n",
    "        self.split = split\n",
    "        self.normalization_type = normalization_type\n",
    "        \n",
    "        # Class mapping - consistent with preprocessing\n",
    "        self.classes = {\n",
    "            'Mild_Demented': 0,\n",
    "            'Moderate_Demented': 1,\n",
    "            'Non_Demented': 2,\n",
    "            'Very_Mild_Demented': 3\n",
    "        }\n",
    "        \n",
    "        # Use the normalized subdirectory matching the dataset organization\n",
    "        self.data_dir = os.path.join(base_dir, \"normalized\", split)\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(f\"Data directory {self.data_dir} does not exist\")\n",
    "            \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Process each class folder\n",
    "        for class_name, label in self.classes.items():\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            \n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"Warning: Class directory {class_dir} does not exist\")\n",
    "                continue\n",
    "                \n",
    "            # Process each image in the class folder\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                # Filter for the specified normalization type\n",
    "                if f\"_{normalization_type}.png\" in img_file.lower():\n",
    "                    img_path = os.path.join(class_dir, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} {normalization_type} images for {split} split\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        class_counts = {}\n",
    "        for label in self.labels:\n",
    "            class_name = CLASS_NAMES[label]\n",
    "            if class_name in class_counts:\n",
    "                class_counts[class_name] += 1\n",
    "            else:\n",
    "                class_counts[class_name] = 1\n",
    "        \n",
    "        print(\"Class distribution:\")\n",
    "        for class_name, count in class_counts.items():\n",
    "            print(f\"  {class_name}: {count} images ({count/len(self.labels)*100:.1f}%)\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path and label\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        img_tensor = TF.to_tensor(img)\n",
    "        \n",
    "        return img_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea52439-84af-4d21-80ae-53b79fbc1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VentricleCNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_size=(128, 128)):\n",
    "        super(VentricleCNN, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Third convolutional block for deeper features\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # After 3 pooling layers of 2x2, dimensions are reduced by factor of 8\n",
    "        reduced_h = input_size[0] // 8\n",
    "        reduced_w = input_size[1] // 8\n",
    "        self.fc_input_size = 256 * reduced_h * reduced_w\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(128, N_CLASSES)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.batchnorm1(self.pool1(F.relu(self.conv1(x))))\n",
    "        \n",
    "        # Second block\n",
    "        x = self.batchnorm2(self.pool2(F.relu(self.conv2(x))))\n",
    "        \n",
    "        # Third block\n",
    "        x = self.batchnorm3(self.pool3(F.relu(self.conv3(x))))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Evaluation function for validation\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    class_correct = [0] * N_CLASSES\n",
    "    class_total = [0] * N_CLASSES\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += (predicted[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = {}\n",
    "    for i in range(N_CLASSES):\n",
    "        if class_total[i] > 0:\n",
    "            class_acc = class_correct[i] / class_total[i]\n",
    "            class_accuracies[CLASS_NAMES[i]] = class_acc\n",
    "    \n",
    "    return accuracy, avg_loss, class_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e74ebd-04d2-495a-b03a-386d32fa132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training pipeline for MRI ventricle analysis...\n",
      "\n",
      "Loading datasets...\n",
      "Loaded 12543 ventricle images for train split\n",
      "Class distribution:\n",
      "  Mild_Demented: 2545 images (20.3%)\n",
      "  Moderate_Demented: 1845 images (14.7%)\n",
      "  Non_Demented: 4480 images (35.7%)\n",
      "  Very_Mild_Demented: 3673 images (29.3%)\n",
      "Loaded 1792 ventricle images for val split\n",
      "Class distribution:\n",
      "  Mild_Demented: 363 images (20.3%)\n",
      "  Moderate_Demented: 264 images (14.7%)\n",
      "  Non_Demented: 640 images (35.7%)\n",
      "  Very_Mild_Demented: 525 images (29.3%)\n",
      "Dataset sizes: Train=12543, Validation=1792\n",
      "\n",
      "=== TRAINING PHASE ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█████| 392/392 [05:19<00:00,  1.23it/s, loss=0.716, acc=55.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 1.0513, Training Accuracy: 0.5582\n",
      "Validation Loss: 0.6341, Validation Accuracy: 0.6769\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.7107\n",
      "  Moderate_Demented: 0.9659\n",
      "  Non_Demented: 0.7703\n",
      "  Very_Mild_Demented: 0.3943\n",
      "✅ New best model saved at epoch 1 with validation accuracy: 0.6769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█████| 392/392 [05:43<00:00,  1.14it/s, loss=0.349, acc=69.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training Loss: 0.6372, Training Accuracy: 0.6982\n",
      "Validation Loss: 0.6910, Validation Accuracy: 0.6607\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.9339\n",
      "  Moderate_Demented: 0.9924\n",
      "  Non_Demented: 0.4203\n",
      "  Very_Mild_Demented: 0.5981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█████| 392/392 [05:40<00:00,  1.15it/s, loss=0.307, acc=78.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training Loss: 0.4728, Training Accuracy: 0.7846\n",
      "Validation Loss: 0.3270, Validation Accuracy: 0.8650\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.8320\n",
      "  Moderate_Demented: 1.0000\n",
      "  Non_Demented: 0.9187\n",
      "  Very_Mild_Demented: 0.7543\n",
      "✅ New best model saved at epoch 3 with validation accuracy: 0.8650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█████| 392/392 [05:50<00:00,  1.12it/s, loss=0.143, acc=86.30%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training Loss: 0.3156, Training Accuracy: 0.8630\n",
      "Validation Loss: 0.1864, Validation Accuracy: 0.9425\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.9614\n",
      "  Moderate_Demented: 1.0000\n",
      "  Non_Demented: 0.9797\n",
      "  Very_Mild_Demented: 0.8552\n",
      "✅ New best model saved at epoch 4 with validation accuracy: 0.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|█████| 392/392 [05:37<00:00,  1.16it/s, loss=0.151, acc=92.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training Loss: 0.1959, Training Accuracy: 0.9203\n",
      "Validation Loss: 0.1315, Validation Accuracy: 0.9459\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.9587\n",
      "  Moderate_Demented: 1.0000\n",
      "  Non_Demented: 0.9953\n",
      "  Very_Mild_Demented: 0.8495\n",
      "✅ New best model saved at epoch 5 with validation accuracy: 0.9459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|█████| 392/392 [05:29<00:00,  1.19it/s, loss=0.118, acc=95.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training Loss: 0.1280, Training Accuracy: 0.9538\n",
      "Validation Loss: 0.0525, Validation Accuracy: 0.9844\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.9725\n",
      "  Moderate_Demented: 1.0000\n",
      "  Non_Demented: 0.9969\n",
      "  Very_Mild_Demented: 0.9695\n",
      "✅ New best model saved at epoch 6 with validation accuracy: 0.9844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███| 392/392 [21:07<00:00,  3.23s/it, loss=0.00501, acc=97.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training Loss: 0.0852, Training Accuracy: 0.9705\n",
      "Validation Loss: 0.0336, Validation Accuracy: 0.9922\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.9945\n",
      "  Moderate_Demented: 1.0000\n",
      "  Non_Demented: 0.9922\n",
      "  Very_Mild_Demented: 0.9867\n",
      "✅ New best model saved at epoch 7 with validation accuracy: 0.9922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████| 392/392 [04:56<00:00,  1.32it/s, loss=0.0967, acc=97.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training Loss: 0.0694, Training Accuracy: 0.9750\n",
      "Validation Loss: 0.0459, Validation Accuracy: 0.9894\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.9945\n",
      "  Moderate_Demented: 1.0000\n",
      "  Non_Demented: 0.9766\n",
      "  Very_Mild_Demented: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|█████| 392/392 [23:42<00:00,  3.63s/it, loss=0.127, acc=98.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training Loss: 0.0449, Training Accuracy: 0.9835\n",
      "Validation Loss: 0.0195, Validation Accuracy: 0.9944\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.9917\n",
      "  Moderate_Demented: 1.0000\n",
      "  Non_Demented: 0.9984\n",
      "  Very_Mild_Demented: 0.9886\n",
      "✅ New best model saved at epoch 9 with validation accuracy: 0.9944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███| 392/392 [07:37<00:00,  1.17s/it, loss=0.0127, acc=97.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training Loss: 0.0677, Training Accuracy: 0.9780\n",
      "Validation Loss: 0.0398, Validation Accuracy: 0.9877\n",
      "Per-class validation accuracy:\n",
      "  Mild_Demented: 0.9862\n",
      "  Moderate_Demented: 1.0000\n",
      "  Non_Demented: 0.9750\n",
      "  Very_Mild_Demented: 0.9981\n",
      "\n",
      "Training completed. Best model was from epoch 9 with validation accuracy: 0.9944\n",
      "\n",
      "Generating training curves plot...\n",
      "\n",
      "Training and evaluation complete!\n",
      "Model trained on ventricle normalization\n",
      "Best model was from epoch 9 with validation accuracy: 0.9944\n"
     ]
    }
   ],
   "source": [
    "# Training function with class metrics\n",
    "def train_model(model, train_loader, val_loader=None, num_epochs=10, learning_rate=0.001, \n",
    "                save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = \".\"  # Default to current directory\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Metrics for tracking performance\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    class_accuracies_history = []\n",
    "    \n",
    "    # For tracking best model\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for i, data in enumerate(progress_bar):\n",
    "            # Get the inputs and labels\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_acc = 100 * correct / total\n",
    "            progress_bar.set_postfix({'loss': loss.item(), 'acc': f'{current_acc:.2f}%'})\n",
    "        \n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Evaluate on validation set if provided\n",
    "        if val_loader:\n",
    "            val_acc, val_loss, class_accs = evaluate_model(model, val_loader, criterion)\n",
    "            val_accuracies.append(val_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            class_accuracies_history.append(class_accs)\n",
    "            \n",
    "            print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "            \n",
    "            # Print per-class accuracy\n",
    "            print(\"Per-class validation accuracy:\")\n",
    "            for class_name, acc in class_accs.items():\n",
    "                print(f\"  {class_name}: {acc:.4f}\")\n",
    "            \n",
    "            # Check if this is the best model so far\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                best_epoch = epoch + 1\n",
    "                \n",
    "                # Save the best model \n",
    "                best_model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "                torch.save({\n",
    "                    'epoch': best_epoch,\n",
    "                    'model_state_dict': best_model_state,\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'validation_accuracy': best_val_acc,\n",
    "                    'validation_loss': val_loss,\n",
    "                    'class_accuracies': class_accs,\n",
    "                }, best_model_path)\n",
    "                \n",
    "                print(f\"✅ New best model saved at epoch {best_epoch} with validation accuracy: {best_val_acc:.4f}\")\n",
    "            \n",
    "            # Update learning rate based on validation performance\n",
    "            scheduler.step(val_loss)  # Using validation loss for scheduler\n",
    "    \n",
    "    print(f\"\\nTraining completed. Best model was from epoch {best_epoch} with validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, train_accuracies, val_losses, val_accuracies, best_epoch, best_val_acc, class_accuracies_history\n",
    "\n",
    "def training_pipeline():\n",
    "    img_size = (128, 128)  # Same as in preprocessing\n",
    "    batch_size = 32\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    normalization_type = 'ventricle'  # can switch it up to test our model: 'norm', 'enhanced', 'ventricle', 'mask'\n",
    "    \n",
    "    models_dir = os.path.join(output_dir, \"models\")\n",
    "    plots_dir = os.path.join(output_dir, \"plots\")\n",
    "    \n",
    "    # Load the processed datasets\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    train_dataset = VentricleDataset(base_dir=base_dir, split='train', normalization_type=normalization_type)\n",
    "    val_dataset = VentricleDataset(base_dir=base_dir, split='val', normalization_type=normalization_type)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"Dataset sizes: Train={len(train_dataset)}, Validation={len(val_dataset)}\")\n",
    "    \n",
    "    # Create and train the model\n",
    "    print(\"\\n=== TRAINING PHASE ===\")\n",
    "    model = VentricleCNN(input_size=img_size)\n",
    "    \n",
    "    # Updated function call with additional return values\n",
    "    model, train_losses, train_accuracies, val_losses, val_accuracies, best_epoch, best_val_acc, class_accs_history = train_model(\n",
    "        model, train_loader, val_loader, num_epochs, learning_rate, save_dir=models_dir\n",
    "    )\n",
    "    \n",
    "    # Plot enhanced training curves with all metrics\n",
    "    print(\"\\nGenerating training curves plot...\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Training and validation loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.axvline(x=best_epoch, color='r', linestyle='--', label=f'Best model (epoch {best_epoch})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Training accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies)\n",
    "    plt.axvline(x=best_epoch, color='r', linestyle='--', label=f'Best model (epoch {best_epoch})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Validation accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies)\n",
    "    plt.axvline(x=best_epoch, color='r', linestyle='--', label=f'Best model (epoch {best_epoch})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Combined metrics\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "    plt.axvline(x=best_epoch, color='r', linestyle='--', label=f'Best model (epoch {best_epoch})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f\"training_curves_{normalization_type}.png\"))\n",
    "    plt.close()\n",
    "        \n",
    "    # Save model information\n",
    "    with open(os.path.join(output_dir, f\"model_info_{normalization_type}.txt\"), 'w') as f:\n",
    "        f.write(f\"Normalization Type: {normalization_type}\\n\")\n",
    "        f.write(f\"Best Epoch: {best_epoch}\\n\")\n",
    "        f.write(f\"Best Validation Accuracy: {best_val_acc:.4f}\\n\")\n",
    "        f.write(\"\\nPer-class Validation Accuracy:\\n\")\n",
    "        for class_name, acc in class_accs_history[best_epoch-1].items():\n",
    "            f.write(f\"  {class_name}: {acc:.4f}\\n\")\n",
    "    \n",
    "    print(\"\\nTraining and evaluation complete!\")\n",
    "    print(f\"Model trained on {normalization_type} normalization\")\n",
    "    print(f\"Best model was from epoch {best_epoch} with validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    return model, best_val_acc\n",
    "\n",
    "def main():\n",
    "    print(\"Starting model training pipeline for MRI ventricle analysis...\")\n",
    "    training_pipeline()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15f98e-332b-4b5b-84a7-eb9cdb87363f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
