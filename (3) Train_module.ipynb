{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deec14e0-67b2-4bf7-9c6f-bad87a35a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "# Constants\n",
    "N_CLASSES = 4\n",
    "CLASS_NAMES = ['Mild_Demented', 'Moderate_Demented', 'Non_Demented', 'Very_Mild_Demented']\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "base_dir = \"Combined_MRI_Dataset\"\n",
    "output_dir = os.path.join(base_dir, \"model_output\")\n",
    "\n",
    "class VentricleDataset(Dataset):\n",
    "    def __init__(self, base_dir, split='train'):\n",
    "        self.data_dir = os.path.join(base_dir, \"normalized\", split)\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                continue\n",
    "                \n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.lower().endswith('mask.png'):\n",
    "                    self.images.append(os.path.join(class_dir, img_file))\n",
    "                    self.labels.append(class_idx)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images for {split} split\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx])\n",
    "        return TF.to_tensor(img), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "class VentricleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Two convolutional blocks\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Flattened size after 2 pooling layers (4x reduction)\n",
    "        self.fc_input_size = 128 * (IMG_SIZE[0]//4) * (IMG_SIZE[1]//4)\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(128, N_CLASSES)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(self.pool1(F.relu(self.conv1(x))))\n",
    "        x = self.batchnorm2(self.pool2(F.relu(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b886000-bbdd-4a7c-8659-41811e7a0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    correct, total, loss = 0, 0, 0\n",
    "    class_correct = [0] * N_CLASSES\n",
    "    class_total = [0] * N_CLASSES\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            outputs = model(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            for label, pred in zip(labels, predicted):\n",
    "                class_correct[label] += (pred == label).item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = loss / len(loader)\n",
    "    class_acc = {CLASS_NAMES[i]: class_correct[i]/class_total[i] \n",
    "                for i in range(N_CLASSES) if class_total[i] > 0}\n",
    "    \n",
    "    return accuracy, avg_loss, class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d6be37-9114-4d73-858d-cc52a16774fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 12543 images for train split\n",
      "Loaded 1792 images for val split\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█████████████████████████████| 392/392 [06:19<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.4965, Train Acc: 0.4729, Val Loss: 0.7987, Val Acc: 0.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█████████████████████████████| 392/392 [06:01<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.7488, Train Acc: 0.6299, Val Loss: 0.6204, Val Acc: 0.6981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█████████████████████████████| 392/392 [04:55<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.5779, Train Acc: 0.7366, Val Loss: 0.4018, Val Acc: 0.8298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█████████████████████████████| 392/392 [06:23<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.3905, Train Acc: 0.8370, Val Loss: 0.2867, Val Acc: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████| 392/392 [2:14:46<00:00, 20.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.2167, Train Acc: 0.9157, Val Loss: 0.2434, Val Acc: 0.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████| 392/392 [6:07:38<00:00, 56.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.1356, Train Acc: 0.9500, Val Loss: 0.1689, Val Acc: 0.9381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████| 392/392 [1:11:45<00:00, 10.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0974, Train Acc: 0.9656, Val Loss: 0.1538, Val Acc: 0.9459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|█████████████████████████████| 392/392 [11:11<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0749, Train Acc: 0.9756, Val Loss: 0.1569, Val Acc: 0.9498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|█████████████████████████████| 392/392 [46:35<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0597, Train Acc: 0.9794, Val Loss: 0.1796, Val Acc: 0.9453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|████████████████████████████| 392/392 [16:21<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0529, Train Acc: 0.9825, Val Loss: 0.1362, Val Acc: 0.9587\n",
      "Saving results...\n",
      "Best validation accuracy: 0.9587\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "    \n",
    "    best_acc = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        val_acc, val_loss, class_acc = evaluate(model, val_loader, criterion)\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        history['train_loss'].append(train_loss/len(train_loader))\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {history['train_loss'][-1]:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, \"best_model.pt\"))\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"training_curves.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    train_set = VentricleDataset(base_dir, 'train')\n",
    "    val_set = VentricleDataset(base_dir, 'val')\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model = VentricleCNN()\n",
    "    model, history = train(model, train_loader, val_loader)\n",
    "    \n",
    "    print(\"Saving results...\")\n",
    "    plot_history(history)\n",
    "    print(f\"Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d424f86-2faf-4e47-a8cf-1cf417f3a140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
