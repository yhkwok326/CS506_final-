{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2235b9a-5cb5-4d81-8126-8c56a2e71026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from PIL import Image\n",
    "\n",
    "# Constants\n",
    "N_CLASSES = 4\n",
    "CLASS_NAMES = ['Mild_Demented', 'Moderate_Demented', 'Non_Demented', 'Very_Mild_Demented']\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "base_dir = \"Combined_MRI_Dataset\"\n",
    "output_dir = os.path.join(base_dir, \"model_output\")\n",
    "\n",
    "class VentricleDataset(Dataset):\n",
    "    def __init__(self, base_dir, split='test'):\n",
    "        self.data_dir = os.path.join(base_dir, \"normalized\", split)\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "            class_dir = os.path.join(self.data_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"Warning: Class directory {class_dir} not found.\")\n",
    "                continue\n",
    "                \n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.lower().endswith('mask.png'):\n",
    "                    self.images.append(os.path.join(class_dir, img_file))\n",
    "                    self.labels.append(class_idx)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images for {split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx])\n",
    "        return TF.to_tensor(img), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b6dce8-eb54-4134-8c1d-ca29be7e94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VentricleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Two convolutional blocks\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Flattened size after 2 pooling layers (4x reduction)\n",
    "        self.fc_input_size = 128 * (IMG_SIZE[0]//4) * (IMG_SIZE[1]//4)\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(128, N_CLASSES)\n",
    "        \n",
    "        # GradCAM attributes\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Hook the last convolutional layer\n",
    "        self.conv2.register_forward_hook(self.save_activations)\n",
    "        self.conv2.register_full_backward_hook(self.save_gradients)\n",
    "    \n",
    "    def save_activations(self, module, input, output):\n",
    "        self.activations = output\n",
    "    \n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(self.pool1(F.relu(self.conv1(x))))\n",
    "        x = self.batchnorm2(self.pool2(F.relu(self.conv2(x))))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        return self.out(x)\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=CLASS_NAMES, digits=4)\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return accuracy, cm, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e3068f4-7235-42c5-8b80-937b4efb5613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 3584 images for test split\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/8_rtb5dd4ng41b3b4_j0zst80000gn/T/ipykernel_53832/3485248474.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location='cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████| 112/112 [00:21<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.9520\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 702    0    6   19]\n",
      " [   0  527    0    0]\n",
      " [  10    0 1203   67]\n",
      " [  20    0   50  980]]\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Mild_Demented     0.9590    0.9656    0.9623       727\n",
      " Moderate_Demented     1.0000    1.0000    1.0000       527\n",
      "      Non_Demented     0.9555    0.9398    0.9476      1280\n",
      "Very_Mild_Demented     0.9193    0.9333    0.9263      1050\n",
      "\n",
      "          accuracy                         0.9520      3584\n",
      "         macro avg     0.9585    0.9597    0.9590      3584\n",
      "      weighted avg     0.9522    0.9520    0.9520      3584\n",
      "\n",
      "Generating visualizations...\n",
      "Saved all results to: Combined_MRI_Dataset/model_output\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    \n",
    "def generate_gradcam(model, input_tensor, target_class=None):\n",
    "    model.eval()\n",
    "    \n",
    "    activations = None\n",
    "    gradients = None\n",
    "    \n",
    "    # Define hooks\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        nonlocal gradients\n",
    "        gradients = grad_out[0]\n",
    "    \n",
    "    def forward_hook(module, input, output):\n",
    "        nonlocal activations\n",
    "        activations = output\n",
    "    \n",
    "    # Register hooks\n",
    "    handle_forward = model.conv2.register_forward_hook(forward_hook)\n",
    "    handle_backward = model.conv2.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(input_tensor.unsqueeze(0))\n",
    "    \n",
    "    if target_class is None:\n",
    "        target_class = torch.argmax(output)\n",
    "    \n",
    "    # Zero gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    torch.sum(output[0, target_class]).backward(retain_graph=True)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    gradients = gradients.detach().cpu().numpy()[0]\n",
    "    activations = activations.detach().cpu().numpy()[0]\n",
    "    \n",
    "    # Important change: Take absolute value of gradients for weighting\n",
    "    # This ensures we consider both positive and negative importance\n",
    "    weights = np.mean(np.abs(gradients), axis=(1, 2))\n",
    "    \n",
    "    # Create the class activation map\n",
    "    # Use absolute activations to capture both positive and negative importance\n",
    "    cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(weights):\n",
    "        # Consider absolute value of activations\n",
    "        cam += w * np.abs(activations[i])\n",
    "\n",
    "    \n",
    "    # No need for ReLU since we're using absolute values\n",
    "    \n",
    "    # Resize to input size\n",
    "    cam = cv2.resize(cam, (input_tensor.shape[2], input_tensor.shape[1]))\n",
    "    \n",
    "    # Normalize\n",
    "    if np.max(cam) > 0:\n",
    "        cam = cam / np.max(cam)\n",
    "    else:\n",
    "        print(\"Warning: Maximum CAM value is 0, cannot normalize\")\n",
    "    \n",
    "    handle_forward.remove()\n",
    "    handle_backward.remove()\n",
    "    \n",
    "    return cam\n",
    "\n",
    "def grad_cam_visualization(model, test_loader, num_samples=5):\n",
    "\n",
    "    os.makedirs(os.path.join(output_dir, \"gradcam\"), exist_ok=True)\n",
    "    \n",
    "    samples = []\n",
    "    for images, labels in test_loader:\n",
    "        for i in range(min(len(images), num_samples)):\n",
    "            samples.append((images[i], labels[i]))\n",
    "        if len(samples) >= num_samples:\n",
    "            break\n",
    "    \n",
    "    # Generate and save Grad-CAM for each sample\n",
    "    for idx, (image, label) in enumerate(samples):\n",
    "        true_class = label.item()\n",
    "        \n",
    "        # Generate Grad-CAM\n",
    "        cam = generate_gradcam(model, image, true_class)\n",
    "        \n",
    "        # Convert image tensor to numpy array for visualization\n",
    "        img_np = image.squeeze().numpy()\n",
    "        \n",
    "        # Create heatmap overlay\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img_np, cmap='gray')\n",
    "        plt.title(f'Original: {CLASS_NAMES[true_class]}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Grad-CAM heatmap\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(cam, cmap='jet')\n",
    "        plt.title('Grad-CAM Heatmap')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(img_np, cmap='gray')\n",
    "        plt.imshow(cam, cmap='jet', alpha=0.5)\n",
    "        plt.title('Overlay')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save figure\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"gradcam\", f\"gradcam_sample_{idx}.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "def load_model(model_path):\n",
    "    model = VentricleCNN()\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    print(\"Model loaded successfully\")\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    test_set = VentricleDataset(base_dir, 'test')\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    model_path = os.path.join(output_dir, \"best_model.pt\")\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    accuracy, cm, report = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(\"Generating visualizations...\")\n",
    "    plot_confusion_matrix(cm)\n",
    "    grad_cam_visualization(model, test_loader)\n",
    "    \n",
    "    print(\"Saved all results to:\", output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c534b0c-5a58-4fa1-bb45-91d12743325f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
